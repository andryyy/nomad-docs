<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Nomad</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="style.css">

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Prepare environment</li><li class="chapter-item expanded "><a href="preparation.html"><strong aria-hidden="true">1.</strong> Preparation</a></li><li class="chapter-item expanded "><a href="basic-setup.html"><strong aria-hidden="true">2.</strong> Basic setup</a></li><li class="chapter-item expanded affix "><li class="part-title">Nomad cluster</li><li class="chapter-item expanded "><a href="nomad/encryption.html"><strong aria-hidden="true">3.</strong> Encryption</a></li><li class="chapter-item expanded "><a href="nomad/services_and_folders.html"><strong aria-hidden="true">4.</strong> Services and folders</a></li><li class="chapter-item expanded "><a href="nomad/server_config.html"><strong aria-hidden="true">5.</strong> Configuration</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="nomad/server_config.html"><strong aria-hidden="true">5.1.</strong> Server agent configuration</a></li><li class="chapter-item expanded "><a href="nomad/client_config.html"><strong aria-hidden="true">5.2.</strong> Client agent configuration</a></li><li class="chapter-item expanded "><a href="nomad/client_agent_access.html"><strong aria-hidden="true">5.3.</strong> Accessing the client</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">System Jobs</li><li class="chapter-item expanded "><a href="system/nginx.html"><strong aria-hidden="true">6.</strong> Nginx</a></li><li class="chapter-item expanded "><a href="system/sshd.html"><strong aria-hidden="true">7.</strong> SSHd</a></li><li class="chapter-item expanded affix "><li class="part-title">Service jobs</li><li class="chapter-item expanded "><a href="service/sync.html"><strong aria-hidden="true">8.</strong> Mutagen</a></li><li class="chapter-item expanded "><a href="service/databases.html"><strong aria-hidden="true">9.</strong> Databases</a></li><li class="chapter-item expanded affix "><li class="part-title">Tools</li><li class="chapter-item expanded "><a href="tools/damon.html"><strong aria-hidden="true">10.</strong> Damon</a></li><li class="chapter-item expanded "><a href="tools/wander.html"><strong aria-hidden="true">11.</strong> Wander</a></li><li class="chapter-item expanded "><a href="tools/lazydocker.html"><strong aria-hidden="true">12.</strong> Lazydocker</a></li><li class="chapter-item expanded affix "><li class="part-title">Scripts</li><li class="chapter-item expanded "><a href="scripts/quick-enter.html"><strong aria-hidden="true">13.</strong> Quick-enter containers</a></li><li class="chapter-item expanded affix "><li class="part-title">Server debug</li><li class="chapter-item expanded "><a href="server_debug/services.html"><strong aria-hidden="true">14.</strong> Nomad services</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Nomad</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- Page table of contents -->
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>

                        <h1 id="description"><a class="header" href="#description">Description</a></h1>
<p>For our Nomad cluster to function properly, we will create three machines, where each machine is installed two agents on: a server and a client.</p>
<p>This is not what either Hashicorp or me recommend for production setups, but read on:</p>
<p>The nomad binary is able to be run as server and client agent started only once. For the sake of a proper configuration we will not stick to a single-configuration setup but create two systemd units representing a client and a server instance. We can bind each service to different internal networks and setup client and server TLS for each of them. The later seems to be more problematic when a single configuration is used as the &quot;tls&quot; stanza does not differentiate between client and server. We are also able to run the server agent with lower privileges and define different service specs.</p>
<p>This is something in between a &quot;dev&quot; and &quot;prod&quot; setup and should be fine for most workloads as long as the machines are being monitored.</p>
<p>But what is a server agent and what is a client agent? &quot;I came here for Kubernetes being too complicated, now look at this&quot; you may wonder.</p>
<p>The concept is actually pretty easy. Server nodes are more or less the brain of the cluster making decisions for clever deployments and orchestration in general.</p>
<p>A client agent is more stupid in this regard and does what it is told by a server agent. A client agent fingerprints the machine it is installed on, monitors resources, spawns containers, virtual machines or protected environments. This is why it requires higher privileges with capabilites like CAP_SYS_ADMIN and CAP_NET_ADMIN. A process running with CAP_SYS_ADMIN capabilites is almost always able to escalate to root, so Nomad client agents use root privileges to begin with.</p>
<p>A Nomad server agent could be placed on a smaller virtual machine while the client must be installed on a machine where workload is deployed. It is best for a client agent to not share/battle resources with other processes that it is not aware of. In theory a broken Nomad server agent could balloon up and kill the client if not configured properly.</p>
<p>Nomad client <strong>and</strong> server agents will join a <strong>server</strong> group.</p>
<p><strong>Networking</strong> between Nomad server agents differs slightly from client agent network requirements: A Nomad server cluster should be able to exchange information with less than 10 ms delay. Client agents do not take part in a quorum and work fine with 100 ms latency and higher.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="preparation"><a class="header" href="#preparation">Preparation</a></h1>
<p>For my setup I spawned three CX11 machines on Hetzer Cloud named nomad-1, nomad-2 and nomad-3.</p>
<p>Each node has a public facing IPv4 and IPv6 address as well as two internal networks for client and server traffic:</p>
<ul>
<li><code>10.100.100.0/24</code> is the network used for Nomad server agents</li>
<li><code>10.200.200.0/24</code> is the network used for Nomad client agents</li>
</ul>
<p>Both networks are <strong>not</strong> isolated from eachother. Our client agents need to talk to our server agents using RPC via <code>4647/tcp</code>.</p>
<p>IPs assigned in these networks must be static.</p>
<p>Nomad server agents mostly use gossip on <code>4648/tcp+udp</code> within their own class for consensus decision-making and other communication.</p>
<p>The network used for Nomad server agent gossip <strong>could be</strong> isolated from the other networks and abstracted as cross-regional network over VPN. Gossip traffic will be encrypted in our setup, a network for this kind of traffic should not add extra latency by using an overly complex encryption.</p>
<p>Besides the machine hostname I will assign two more names to each host:</p>
<pre><code class="language-yaml">Machine hostname nomad-1:
  DNS:
    - Server name in cluster: server-1.nomad.cluster
    - Client name in cluster: client-1.nomad.cluster

Machine hostname nomad-2:
  DNS:
    - Server name in cluster: server-2.nomad.cluster
    - Client name in cluster: client-2.nomad.cluster

Machine hostname nomad-3:
  DNS:
    - Server name in cluster: server-3.nomad.cluster
    - Client name in cluster: client-3.nomad.cluster
</code></pre>
<p>A simple for-loop will setup hostnames and modify the hosts file accordingly in the next steps.</p>
<p>This is my temporary ssh config file. I write &quot;temporary&quot; as I do not prefer to use root as default user for anything, but it will help a lot to create, append, or copy configurations between nodes while setting up the cluster:</p>
<pre><code class="language-yaml">Host nomad-1
  User root
  Hostname 5.75.230.14
  Port 22
  LocalForward 127.0.0.1:4646 10.100.100.2:4646
  ForwardAgent yes

Host nomad-2
  User root
  Hostname 5.75.230.15
  LocalForward 127.0.0.1:4647 10.100.100.3:4646
  Port 22
  ForwardAgent yes

Host nomad-3
  User root
  Hostname 5.75.230.16
  LocalForward 127.0.0.1:4648 10.100.100.4:4646
  Port 22
  ForwardAgent yes
</code></pre>
<p>As you see I will use agent forwarding. I will be able to jump from/to each worker using my forwarded agent, that is as long as I carry the SSH agent of course. You can do as you like.</p>
<p>To access the web UI I added a port forwarding to expose the server agents HTTP listener to my local machine. We will also encrypt HTTP traffic and require to authenticate using a client certificate.</p>
<hr />
<p>To make life easier I will use JSON in combination with &quot;jq&quot; to pre-seed a set of variables to use in different occasions. I will write this data to <code>~/nomad-env.json</code>:</p>
<pre><code class="language-json">{
  &quot;nomad-1&quot;: {
    &quot;server_name&quot;: &quot;server-1.nomad.cluster&quot;,
    &quot;client_name&quot;: &quot;client-1.nomad.cluster&quot;,
    &quot;server&quot;: &quot;10.100.100.2&quot;,
    &quot;client&quot;: &quot;10.200.200.2&quot;,
    &quot;server_interface&quot;: &quot;ens10&quot;,
    &quot;client_interface&quot;: &quot;ens11&quot;
  },
  &quot;nomad-2&quot;: {
    &quot;server_name&quot;: &quot;server-2.nomad.cluster&quot;,
    &quot;client_name&quot;: &quot;client-2.nomad.cluster&quot;,
    &quot;server&quot;: &quot;10.100.100.3&quot;,
    &quot;client&quot;: &quot;10.200.200.3&quot;,
    &quot;server_interface&quot;: &quot;ens10&quot;,
    &quot;client_interface&quot;: &quot;ens11&quot;
  },
  &quot;nomad-3&quot;: {
    &quot;server_name&quot;: &quot;server-3.nomad.cluster&quot;,
    &quot;client_name&quot;: &quot;client-3.nomad.cluster&quot;,
    &quot;server&quot;: &quot;10.100.100.4&quot;,
    &quot;client&quot;: &quot;10.200.200.4&quot;,
    &quot;server_interface&quot;: &quot;ens10&quot;,
    &quot;client_interface&quot;: &quot;ens11&quot;
  }
}
</code></pre>
<p>Now <strong>validate</strong> the correct evaluation using &quot;jq&quot;:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  cat &lt;&lt; TEST

Hostname ${nomad}:
  - Server name in cluster: $(jq -r &quot;.\&quot;$nomad\&quot;.server_name&quot; ~/nomad-env.json)
  - Client name in cluster: $(jq -r &quot;.\&quot;$nomad\&quot;.client_name&quot; ~/nomad-env.json)
  - Client address $(jq -r &quot;.\&quot;$nomad\&quot;.client&quot; ~/nomad-env.json) on interface $(jq -r &quot;.\&quot;$nomad\&quot;.client_interface&quot; ~/nomad-env.json)
  - Server address $(jq -r &quot;.\&quot;$nomad\&quot;.server&quot; ~/nomad-env.json) on interface $(jq -r &quot;.\&quot;$nomad\&quot;.server_interface&quot; ~/nomad-env.json)

TEST
done
</code></pre>
<p>This is a great opportunity to get an overview of the setup:</p>
<pre><code>Hostname nomad-1:
  - Server name in cluster: server-1.nomad.cluster
  - Client name in cluster: client-1.nomad.cluster
  - Client address 10.200.200.2 on interface ens11
  - Server address 10.100.100.2 on interface ens10


Hostname nomad-2:
  - Server name in cluster: server-2.nomad.cluster
  - Client name in cluster: client-2.nomad.cluster
  - Client address 10.200.200.3 on interface ens11
  - Server address 10.100.100.3 on interface ens10


Hostname nomad-3:
  - Server name in cluster: server-3.nomad.cluster
  - Client name in cluster: client-3.nomad.cluster
  - Client address 10.200.200.4 on interface ens11
  - Server address 10.100.100.4 on interface ens10
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="basic-setup"><a class="header" href="#basic-setup">Basic setup</a></h1>
<p>I will start by setting the hostname, timezone, and populating the node hostnames (manually). My scripts will be run using a bash shell.</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;EOF
    hostnamectl set-hostname $nomad;
    timedatectl set-timezone Europe/Berlin;
    cat &lt;&lt; HOSTS &gt;&gt; /etc/hosts
# Nomad server agents
10.100.100.2 server-1.nomad.cluster
10.100.100.3 server-2.nomad.cluster
10.100.100.4 server-3.nomad.cluster
# Nomad client agents
10.200.200.2 client-1.nomad.cluster
10.200.200.3 client-2.nomad.cluster
10.200.200.4 client-3.nomad.cluster
HOSTS
    apt install jq -y
EOF
done
</code></pre>
<p>Some basic aliases; you may skip this part.</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
cat &lt;&lt;'ALIASES'&gt; ~/.bashrc
export LS_OPTIONS='--color=auto'
eval &quot;$(dircolors)&quot;
alias ls='ls $LS_OPTIONS'
alias ll='ls $LS_OPTIONS -la'
alias l='ls $LS_OPTIONS -lA'
ALIASES
EOF
done
</code></pre>
<p>The Docker driver will be used, so Docker needs to be installed on each Nomad client. Nomad will automatically detect the driver.</p>
<p>This is a lazy approach to install Docker. Piping shell scripts from the internet is never a good idea, keep that in mind.</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
  curl -fsSL https://get.docker.com | sh
EOF
done
</code></pre>
<p>Let's add the Hashicorp repository and install Nomad. The default service will be stopped (if running) and disabled, autocomplete for Nomad is installed:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor &gt; /usr/share/keyrings/hashicorp-archive-keyring.gpg
    echo &quot;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main&quot; &gt; /etc/apt/sources.list.d/hashicorp.list
    apt update
    apt install nomad -y
    systemctl stop nomad.service
    systemctl disable nomad.service
    systemctl mask nomad.service
    nomad -autocomplete-install
EOF
done
</code></pre>
<p>Now for the last requirement I will download and extract the CNI plugins to <code>/opt/cni/bin</code> where they will be picked up by Nomad automatically.</p>
<p>The CNI plugins are necessary for the &quot;nomad&quot; bridge to be created.</p>
<p>The &quot;port bindings&quot; created in a bridged network mode are solely DNAT'ed to their dynamic destination, this is a concept I was not aware of when starting with Nomad.</p>
<p>We will not see a listener on that port using <code>ss</code> or <code>netstat</code>, instead we can call <code>iptables -L CNI-HOSTPORT-DNAT -t nat -n</code> to check for their existence. Running the command <em>now</em> will result in either an error or an empty return as there is no chain available by that name.</p>
<p>Version 1.1.1 may be deprecated by the time of reading:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    mkdir -p /opt/cni/bin
    curl -L -o cni-plugins.tgz https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz
    tar -C /opt/cni/bin -xzf cni-plugins.tgz
EOF
done
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encryption"><a class="header" href="#encryption">Encryption</a></h1>
<h2 id="gossip-server-nodes"><a class="header" href="#gossip-server-nodes">Gossip (server nodes)</a></h2>
<p>Gossip is traffic between server nodes on port 4648/tcp/udp. Gossip traffic is not read by a Nomad client. Gossip is encrypted by using symmetric encryption, so all server nodes will share the same secret given as base64 formatted string. The maximum length is 32 bytes.</p>
<p>I will generate random bytes for gossip encryption between Nomad servers when populating the server configuration file.</p>
<h2 id="mtls-encryption-for-http-and-rpc-traffic"><a class="header" href="#mtls-encryption-for-http-and-rpc-traffic">mTLS encryption for HTTP and RPC traffic</a></h2>
<p>All nodes, no matter their role, talk RPC (4647/tcp) and HTTP (4646/tcp) traffic.</p>
<p>HTTP is obviously used for the web UI but also necessary for running nomad commands via terminal as Nomad is fully API-driven - as most (all?) Hashicorp products are.</p>
<p>RPC is used for communication between clients and servers.</p>
<h3 id="preface-how-does-tls-in-nomad-work"><a class="header" href="#preface-how-does-tls-in-nomad-work">Preface: How does TLS in Nomad work?</a></h3>
<p>It is helpful to understand the basics of what Nomad actually verifies using which name using which protocol.</p>
<h4 id="rpc"><a class="header" href="#rpc">RPC</a></h4>
<p>For <strong>RPC communication</strong> Nomad agents will use a pseudo name not resolved in DNS depending on the agent role:</p>
<ul>
<li><code>client.$region.nomad</code></li>
<li><code>server.$region.nomad</code></li>
</ul>
<p><strong>Quick note</strong>: The default value for &quot;region&quot; is &quot;global&quot;, I will get back to this later. <code>$region</code> is merely a placeholder here.</p>
<p>There are two modes for verification where &quot;verify_server_hostname&quot; can be either &quot;true&quot; or &quot;false&quot;.</p>
<p>&quot;verify_server_hostname&quot; set to <strong>false</strong> will only require the cluster to use certificates signed by the same CA.</p>
<p>&quot;verify_server_hostname&quot; set to <strong>true</strong> requires not only the CA but also the region to match. A Nomad agent using <code>server.us-west.nomad</code> would not be able to join a cluster in the region <code>eu-west</code> for example.</p>
<p>In theory a single certificate containing both client and server pseudo names would be sufficient for Nomad no matter the agent role. You don't <strong>have to</strong> add more names for a functional setup, but read on...</p>
<hr />
<h4 id="http"><a class="header" href="#http">HTTP</a></h4>
<p>HTTP as the second component covered by mTLS will use the <strong>same certificate as RPC</strong>.</p>
<p>TLS can be enabled independently for RPC and HTTP, but only one shared certificate can be defined for both services.</p>
<p><strong>Valid certificates for the HTTP endpoints should be preferred.</strong>
The certificates will include the server as well as client agents name as populated in DNS, i.e. &quot;client-1.nomad.cluster&quot; and  &quot;server-1.nomad.cluster&quot;.</p>
<p>Breaking the certificates down to only include the pseudo name (i.e. &quot;client.global.nomad&quot;) and the specific name of the agent (i.e. &quot;client-1.nomad.cluster&quot;) does not offer higher security compared to a combined certificate. As of writing this documentation Nomad is not able to read a CRL to invalidate certificates automatically, but in an existing PR (4901) the developers discussed the implementation of such a mechanism.</p>
<p>One combined certificate for all client agents and one for all server agents is fine.</p>
<p>A powerful access control for the HTTP endpoint is setting <code>verify_https_client</code> to <strong>true</strong> and enforce a policy to require a valid client certificate signed by the same CA. All requests to the HTTP endpoint including the API are covered. When set to <strong>false</strong>, the channel is encrypted but there is no mechanism to restrict.</p>
<p><strong>Note</strong>: Creating and importing a client certificate as &quot;.p12&quot; file into the browser is explained in the course of this document.</p>
<p>As long as CRLs are not supported, short-lived client certificates might be something to look into.</p>
<p>A better and more granular method to restrict access in general is using Nomads ACL system, which can be quite complex.</p>
<h4 id="our-setup"><a class="header" href="#our-setup">Our setup</a></h4>
<p>This documentation will follow an easy to reproduce scheme:</p>
<ul>
<li>
<p><strong>One</strong> certificate for all Nomad client agents:</p>
<ul>
<li>Hostnames in certificate: <code>client.global.nomad</code>, <code>*.nomad.cluster</code></li>
</ul>
</li>
<li>
<p><strong>One</strong> certificate for all Nomad server agents:</p>
<ul>
<li>Hostnames in certificate: <code>server.global.nomad</code>, <code>*.nomad.cluster</code></li>
</ul>
</li>
<li>
<p>A client certificate to authenticate against Nomads HTTP endpoint</p>
</li>
</ul>
<p>As wildcard certificates are supported by Nomad, we will make use of that.</p>
<p><strong>Important</strong>: &quot;server.global.nomad&quot; is used to address any server agent in the region &quot;global&quot;. This is the default region in Nomad we will adopt to. Changing the region to something like <code>eu-west</code> would require to append <code>server.eu-west.nomad</code> as hostname. This setup will validate the region.</p>
<h3 id="bootstrap-a-minimal-ca"><a class="header" href="#bootstrap-a-minimal-ca">Bootstrap a minimal CA</a></h3>
<p>I will use nomad-1 to bootstrap a minimal CA using &quot;cfssl&quot; as described in the Nomad documentation:</p>
<pre><code class="language-bash">root@nomad-1:~ # apt install golang-cfssl
root@nomad-1:~ # mkdir /etc/nomad.d/pki ; cd /etc/nomad.d/pki
root@nomad-1:/etc/nomad.d/pki #
</code></pre>
<p>Create the CA with default values:</p>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # cfssl print-defaults csr | cfssl gencert -initca - | cfssljson -bare nomad-ca
</code></pre>
<p>At this point the CA is alive.</p>
<p>A certificate template is created, the fields should be pretty self-explanatory:</p>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # cat &lt;&lt;EOF&gt; /etc/nomad.d/pki/cfssl.json
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;,
      &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;]
    }
  }
}
EOF
</code></pre>
<h3 id="server-agent-certificate"><a class="header" href="#server-agent-certificate">Server agent certificate</a></h3>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # echo '{}' | cfssl gencert \
  -ca=/etc/nomad.d/pki/nomad-ca.pem \
  -ca-key=/etc/nomad.d/pki/nomad-ca-key.pem \
  -config=/etc/nomad.d/pki/cfssl.json \
  -hostname=&quot;server.global.nomad,*.nomad.cluster&quot; - | cfssljson -bare server
</code></pre>
<h3 id="client-agent-certificate"><a class="header" href="#client-agent-certificate">Client agent certificate</a></h3>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # echo '{}' | cfssl gencert \
  -ca=/etc/nomad.d/pki/nomad-ca.pem \
  -ca-key=/etc/nomad.d/pki/nomad-ca-key.pem \
  -config=/etc/nomad.d/pki/cfssl.json \
  -hostname=&quot;client.global.nomad,*.nomad.cluster&quot; - | cfssljson -bare client
</code></pre>
<h3 id="client-authentication-certificate"><a class="header" href="#client-authentication-certificate">Client authentication certificate</a></h3>
<p>This certificate does not contain hostnames and is solely used to authenticate to the HTTP endpoint:</p>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # echo '{}' | cfssl gencert -ca=nomad-ca.pem -ca-key=nomad-ca-key.pem -profile=client \
  - | cfssljson -bare cli
</code></pre>
<h3 id="seeding-certificates-cleanup-and-details"><a class="header" href="#seeding-certificates-cleanup-and-details">Seeding certificates, cleanup, and details</a></h3>
<p>Change the owner to nomad and its default group:</p>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # chown -R nomad: /etc/nomad.d/pki
</code></pre>
<p>Now let's copy the pki data to all nodes. <strong>You should move the CA key to a host outside the Nomad cluster.</strong></p>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # scp -r /etc/nomad.d/pki server-2.nomad.cluster:/etc/nomad.d/ ; ssh server-2.nomad.cluster chown -R nomad: /etc/nomad.d/pki
root@nomad-1:/etc/nomad.d/pki # scp -r /etc/nomad.d/pki server-3.nomad.cluster:/etc/nomad.d/ ; ssh server-3.nomad.cluster chown -R nomad: /etc/nomad.d/pki
</code></pre>
<p>Before populating the server and client configuration files, we will export some variables to communicate with the corresponding HTTP server of the local Nomad server agent.</p>
<p>It does not matter wether or not Nomad is running at this point.</p>
<p>We are using the server agents DNS name as NOMAD_ADDR. These names are part of the created server certificate.</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;EOF
cat &lt;&lt;PROFILE&gt;&gt; ~/.profile
export NOMAD_ADDR=https://$(jq -r &quot;.\&quot;$nomad\&quot;.server_name&quot; ~/nomad-env.json):4646
export NOMAD_CACERT=/etc/nomad.d/pki/nomad-ca.pem
export NOMAD_CLIENT_CERT=/etc/nomad.d/pki/cli.pem
export NOMAD_CLIENT_KEY=/etc/nomad.d/pki/cli-key.pem
PROFILE
EOF
done
</code></pre>
<h3 id="creating-a-p12-file"><a class="header" href="#creating-a-p12-file">Creating a .p12 file</a></h3>
<p>I do want to access the Nomad web UI with my browser, so the CLI certificate must be imported into my local Firefox.</p>
<p>A proper &quot;.p12&quot; file should be password protected. Some browsers or operation systems will refuse to import a &quot;.p12&quot; file without a password set (see iOS).</p>
<pre><code class="language-bash">root@nomad-1:/etc/nomad.d/pki # openssl pkcs12 -export \
  -in cli.pem \
  -inkey cli-key.pem \
  -out nomad-cli.p12 \
  -name &quot;Nomad CLI&quot;
</code></pre>
<p><strong>Important</strong>: You may find yourself not being able to import the client certificate in iOS when using OpenSSL &gt;= v3.
To be able to create an importable file, you need to append the <code>-legacy</code> flag to the command above.</p>
<p>Scrolling back to the top you may remember the port forwarding added to my <code>.ssh/config</code> file. That's how I will be able to access the UI whenever I need to.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="services-and-folders"><a class="header" href="#services-and-folders">Services and folders</a></h1>
<p>Nomad will be running as server and client agent on the same host. By default a Nomad service file will run the agent as root, no matter its purpose: being a client or a server.</p>
<p>Nomad recommends to run the client agent as root while protecting its data store from unprivileged access.
A client agent needs to spawn containers, create protected environments (&quot;chroot&quot;) and do some other low level operations which mostly require root(-like) access. It is not impossible to use a non-root user, but that will require thoroughly testing.</p>
<p>The server agent should run with lowest privileges with full access to its data directory.</p>
<p>Another consideration is to remove the OOM score from the server agent as well as any limit adjustments while keeping them active for the client.</p>
<p>It is very important for the client not to be killed due to other processes claiming system resources it has no control over. The most important reason a client should not to be placed on the same node as a server is unpredicted availability of system resources. This will not apply to all workloads and also depends a lot on the expected usage in relation to available resources.
Preventing it from being killed is not a solution to the problem, but it will help the cluster to notice a problem and act on it.</p>
<h2 id="nomad-data-folder-structure"><a class="header" href="#nomad-data-folder-structure">Nomad data folder structure</a></h2>
<p>Making sure to be using a proper Nomad data folder structure includes setting the ownership and permissions accordingly for each agent configuration. This step is necessary due to the nature of a default Nomad installation to be run as root no matter what. Default data directories can be problematic.</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    # This may be dangerous, validate you are not removing a directory in production!
    rm -rf /opt/nomad/data
    # (Re-)Create the folder structure
    mkdir -p /opt/nomad/data/{server,client}
    # All belong to nomad
    chown -R nomad: /opt/nomad
    # Except for client data, we can lock this folder up
    chown root: /opt/nomad/data/client
    chmod 700 /opt/nomad/data/client
EOF
done
</code></pre>
<h2 id="create-a-host-volume-on-all-client-agents"><a class="header" href="#create-a-host-volume-on-all-client-agents">Create a host volume on all client agents</a></h2>
<p>For host volumes used in any Nomad job we will create a new directory <code>/opt/nomad-vols</code> containing <code>shared-data</code> for the directory synchronized by Mutagen:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    mkdir -p /opt/nomad-vols/shared-data;
    chown root: /opt/nomad-vols
EOF
done
</code></pre>
<p><strong>Hint</strong>: Using a host volume in a protected environment (&quot;chroot&quot;) requires permissions to be set to nobody:nogroup for the task to be able to write to it. There is no need to act now, this is just a friendly note to remember.</p>
<p>A host volume is defined in a client configuration.</p>
<h2 id="server-agent-service-files"><a class="header" href="#server-agent-service-files">Server agent service files</a></h2>
<p>Create the service files for the <strong>server</strong> agents on each machine:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    cat &lt;&lt;'SERVICE' &gt; /etc/systemd/system/nomad-server.service
[Unit]
Description=Nomad server
Documentation=https://nomadproject.io/docs/
Wants=network-online.target
After=network-online.target

[Service]
ExecReload=/bin/kill -HUP $MAINPID
ExecStart=/usr/bin/nomad agent -config /etc/nomad.d/server.hcl
User=nomad
Group=nomad
Restart=on-failure
RestartSec=2
KillMode=process
KillSignal=SIGINT

[Install]
WantedBy=multi-user.target
SERVICE
EOF
done
</code></pre>
<h2 id="client-agent-service-files"><a class="header" href="#client-agent-service-files">Client agent service files</a></h2>
<p>Create the service files for the <strong>client</strong> agents on each machine:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    cat &lt;&lt;'SERVICE' &gt; /etc/systemd/system/nomad-client.service
[Unit]
Description=Nomad client
Documentation=https://nomadproject.io/docs/
Wants=network-online.target
After=network-online.target

[Service]
ExecReload=/bin/kill -HUP $MAINPID
ExecStart=/usr/bin/nomad agent -config /etc/nomad.d/client.hcl
Restart=on-failure
RestartSec=2
KillMode=process
KillSignal=SIGINT
LimitNOFILE=65536
LimitNPROC=infinity
TasksMax=infinity
OOMScoreAdjust=-1000

[Install]
WantedBy=multi-user.target
SERVICE
EOF
done
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="server-agent-configuration"><a class="header" href="#server-agent-configuration">Server agent configuration</a></h1>
<p>Let's start with the server configuration. In case you wonder and missed the hint in the previous chapter: there is no need to define the host volume in a server agent configuration.</p>
<p>The server agent will be aware of the capabilities of a client and delegate tasks to matching client agents. A client missing a host volume will not be taken into consideration for a deployment.</p>
<p><strong>Hint:</strong> See the <code>gossip_key</code> variable, don't miss it.</p>
<pre><code class="language-bash">gossip_key=$(openssl rand -base64 32)
for nomad in $(jq -r keys[] ~/nomad-env.json); do
  server_ip=$(jq -r &quot;.\&quot;$nomad\&quot;.server&quot; ~/nomad-env.json)
  ssh $nomad bash &lt;&lt;EOF
    cat &lt;&lt;SERVER &gt; /etc/nomad.d/server.hcl
# data_dir will be suffixed by the agent configuration automatically: server/ or client/
data_dir = &quot;/opt/nomad/data&quot;

# We deploy in Falkenstein, this is only important in job files to be created later on.
datacenter = &quot;falkenstein&quot;

# All services of the server agent will bind to the bind_addr defined IP address: rpc, http and serf (gossip)
# The bind_addr can be overruled by the addresses stanza where each service IP binding can be defined by itself.
# Ports can be defined using the ports stanza.
# It seems like you also need to define an advertise stanza in conjunction with the addresses stanza.
# This is especially helpful with NAT: the advertise stanza also accepts ports.
# For a better understanding we will use bind_addr, ports, addresses and advertise stanzas using all default ports.
# This will do exactly the same as only defining bind_addr, I just want to make you aware of what is possible:
# Note: External IPs are being discovered by the network fingerprinting mechanism in Nomad.

bind_addr = &quot;${server_ip}&quot;
ports {
  http = 4646
  rpc  = 4647
  serf = 4648
}
advertise {
  http = &quot;${server_ip}:4646&quot;
  rpc  = &quot;${server_ip}:4647&quot;
  serf = &quot;${server_ip}:4648&quot;
}
addresses {
  http = &quot;${server_ip}&quot;
  rpc  = &quot;${server_ip}&quot;
  serf = &quot;${server_ip}&quot;
}

server {
  # Yes, we are a server agent
  enabled = true
  # We will only bootstrap the cluster when all server agents can gossip
  bootstrap_expect = 3
  server_join {
    # Our join mechanism is to retry until success. We can define ourself here, too, that's fine.
    # Use hostnames as defined in the certificate!
    retry_join = [&quot;server-1.nomad.cluster&quot;, &quot;server-2.nomad.cluster&quot;, &quot;server-3.nomad.cluster&quot;]
  }
  # The key for gossip encryption created previously using openssl.
  # The value is the same for all nodes obviously.
  encrypt = &quot;${gossip_key}&quot;
}

client {
  # Not a client agent
  enabled = false
}

tls {
  # We do want to encrypt RPC traffic
  rpc = true
  # We do want to encrypt HTTP traffic
  http = true
  ca_file = &quot;/etc/nomad.d/pki/nomad-ca.pem&quot;
  cert_file = &quot;/etc/nomad.d/pki/server.pem&quot;
  key_file = &quot;/etc/nomad.d/pki/server-key.pem&quot;

  # verify_server_hostname will require the role (&quot;client&quot;, &quot;server&quot;) and region (&quot;global&quot;) to be verified.
  # Setting it to false results in only the CA to be verified.
  # Servers from other or unwanted regions could join a cluster when they should not be allowed to.
  verify_server_hostname = true

  # We do only want to allow access to the HTTP API using a client certificate.
  # Affects curl, Nomad CLI, and others
  # We will catch this up later.
  verify_https_client = true
}
SERVER
EOF
done
</code></pre>
<p>After validating the server configuration we are now able to enable and start the server agent service:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    systemctl enable --now nomad-server.service;
EOF
done
</code></pre>
<p>I can now login to one of my three Nomad machines and check wether a cluster leader was elected:</p>
<pre><code class="language-bash">root@nomad-1:~# nomad server members
Name            Address       Port  Status  Leader  Raft Version  Build  Datacenter   Region
nomad-1.global  10.100.100.2  4648  alive   false   3             1.4.3  falkenstein  global
nomad-2.global  10.100.100.3  4648  alive   true    3             1.4.3  falkenstein  global
nomad-3.global  10.100.100.4  4648  alive   false   3             1.4.3  falkenstein  global
</code></pre>
<p>Sometimes you may find yourself in a situation where a server agent does not feel like joining the gang.</p>
<p>Often it is a matter of an existing Nomad data directory bootstrapped from a previous configuration. Since we are just starting a new cluster, we can safely stop the service and remove the data directories. Keep in mind to recreate it with proper permission and ownership as described in the previous chapter.</p>
<p>Whatever you decide to do: your first contact point should always be the log files: <code>journalctl -u nomad-server -f</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="server-agent-configuration-1"><a class="header" href="#server-agent-configuration-1">Server agent configuration</a></h1>
<p>Let's start with the server configuration. In case you wonder and missed the hint in the previous chapter: there is no need to define the host volume in a server agent configuration.</p>
<p>The server agent will be aware of the capabilities of a client and delegate tasks to matching client agents. A client missing a host volume will not be taken into consideration for a deployment.</p>
<p><strong>Hint:</strong> See the <code>gossip_key</code> variable, don't miss it.</p>
<pre><code class="language-bash">gossip_key=$(openssl rand -base64 32)
for nomad in $(jq -r keys[] ~/nomad-env.json); do
  server_ip=$(jq -r &quot;.\&quot;$nomad\&quot;.server&quot; ~/nomad-env.json)
  ssh $nomad bash &lt;&lt;EOF
    cat &lt;&lt;SERVER &gt; /etc/nomad.d/server.hcl
# data_dir will be suffixed by the agent configuration automatically: server/ or client/
data_dir = &quot;/opt/nomad/data&quot;

# We deploy in Falkenstein, this is only important in job files to be created later on.
datacenter = &quot;falkenstein&quot;

# All services of the server agent will bind to the bind_addr defined IP address: rpc, http and serf (gossip)
# The bind_addr can be overruled by the addresses stanza where each service IP binding can be defined by itself.
# Ports can be defined using the ports stanza.
# It seems like you also need to define an advertise stanza in conjunction with the addresses stanza.
# This is especially helpful with NAT: the advertise stanza also accepts ports.
# For a better understanding we will use bind_addr, ports, addresses and advertise stanzas using all default ports.
# This will do exactly the same as only defining bind_addr, I just want to make you aware of what is possible:
# Note: External IPs are being discovered by the network fingerprinting mechanism in Nomad.

bind_addr = &quot;${server_ip}&quot;
ports {
  http = 4646
  rpc  = 4647
  serf = 4648
}
advertise {
  http = &quot;${server_ip}:4646&quot;
  rpc  = &quot;${server_ip}:4647&quot;
  serf = &quot;${server_ip}:4648&quot;
}
addresses {
  http = &quot;${server_ip}&quot;
  rpc  = &quot;${server_ip}&quot;
  serf = &quot;${server_ip}&quot;
}

server {
  # Yes, we are a server agent
  enabled = true
  # We will only bootstrap the cluster when all server agents can gossip
  bootstrap_expect = 3
  server_join {
    # Our join mechanism is to retry until success. We can define ourself here, too, that's fine.
    # Use hostnames as defined in the certificate!
    retry_join = [&quot;server-1.nomad.cluster&quot;, &quot;server-2.nomad.cluster&quot;, &quot;server-3.nomad.cluster&quot;]
  }
  # The key for gossip encryption created previously using openssl.
  # The value is the same for all nodes obviously.
  encrypt = &quot;${gossip_key}&quot;
}

client {
  # Not a client agent
  enabled = false
}

tls {
  # We do want to encrypt RPC traffic
  rpc = true
  # We do want to encrypt HTTP traffic
  http = true
  ca_file = &quot;/etc/nomad.d/pki/nomad-ca.pem&quot;
  cert_file = &quot;/etc/nomad.d/pki/server.pem&quot;
  key_file = &quot;/etc/nomad.d/pki/server-key.pem&quot;

  # verify_server_hostname will require the role (&quot;client&quot;, &quot;server&quot;) and region (&quot;global&quot;) to be verified.
  # Setting it to false results in only the CA to be verified.
  # Servers from other or unwanted regions could join a cluster when they should not be allowed to.
  verify_server_hostname = true

  # We do only want to allow access to the HTTP API using a client certificate.
  # Affects curl, Nomad CLI, and others
  # We will catch this up later.
  verify_https_client = true
}
SERVER
EOF
done
</code></pre>
<p>After validating the server configuration we are now able to enable and start the server agent service:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    systemctl enable --now nomad-server.service;
EOF
done
</code></pre>
<p>I can now login to one of my three Nomad machines and check wether a cluster leader was elected:</p>
<pre><code class="language-bash">root@nomad-1:~# nomad server members
Name            Address       Port  Status  Leader  Raft Version  Build  Datacenter   Region
nomad-1.global  10.100.100.2  4648  alive   false   3             1.4.3  falkenstein  global
nomad-2.global  10.100.100.3  4648  alive   true    3             1.4.3  falkenstein  global
nomad-3.global  10.100.100.4  4648  alive   false   3             1.4.3  falkenstein  global
</code></pre>
<p>Sometimes you may find yourself in a situation where a server agent does not feel like joining the gang.</p>
<p>Often it is a matter of an existing Nomad data directory bootstrapped from a previous configuration. Since we are just starting a new cluster, we can safely stop the service and remove the data directories. Keep in mind to recreate it with proper permission and ownership as described in the previous chapter.</p>
<p>Whatever you decide to do: your first contact point should always be the log files: <code>journalctl -u nomad-server -f</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="client-agent-configuration"><a class="header" href="#client-agent-configuration">Client agent configuration</a></h1>
<p>The client agent configuration is a bit more technical. As described in the beginning a client agent deploys workloads on the machine it is installed on.</p>
<p>The client is able to fingerprint most of the hosts capabilities, but it can obvioulsy not know about host volumes you want to expose to tasks or detect networks and assign them roles.</p>
<p>This time we will only be using bind_addr and remove redundant comments in the configuration file.</p>
<p><strong>Important</strong>: In the configuration below I made the Nomad client aware of two host networks:</p>
<ol>
<li>&quot;nomad-clients&quot; via 10.200.200.0/24</li>
<li>&quot;nomad-servers&quot; via 10.200.200.0/24</li>
</ol>
<p>Revalidate the correct interfaces in the JSON config file created in the beginning. The interface must match the host interface on the specific machine. The interface names do not have to be the same for all machines, the names must match though.</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;EOF
  cat &lt;&lt;CLIENT &gt; /etc/nomad.d/client.hcl
data_dir = &quot;/opt/nomad/data&quot;
datacenter = &quot;falkenstein&quot;
bind_addr = &quot;$(jq -r &quot;.\&quot;$nomad\&quot;.client&quot; ~/nomad-env.json)&quot;

server {
  enabled = false
}

client {
  enabled = true
  server_join {
    retry_join = [&quot;server-1.nomad.cluster&quot;, &quot;server-2.nomad.cluster&quot;, &quot;server-3.nomad.cluster&quot;]
  }
  host_volume &quot;shared-data&quot; {
    path = &quot;/opt/nomad-vols/shared-data&quot;
    read_only = false
  }
  host_network &quot;nomad-clients&quot; {
    interface = &quot;$(jq -r &quot;.\&quot;$nomad\&quot;.client_interface&quot; ~/nomad-env.json)&quot;
  }
  host_network &quot;nomad-servers&quot; {
    interface = &quot;$(jq -r &quot;.\&quot;$nomad\&quot;.server_interface&quot; ~/nomad-env.json)&quot;
  }
}

tls {
  rpc = true
  http = true
  ca_file = &quot;/etc/nomad.d/pki/nomad-ca.pem&quot;
  cert_file = &quot;/etc/nomad.d/pki/client.pem&quot;
  key_file = &quot;/etc/nomad.d/pki/client-key.pem&quot;
  verify_server_hostname = true
  verify_https_client = true
}
CLIENT
EOF
done
</code></pre>
<p>Enable and start the client agents <strong>after validating the client configuration</strong>:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
    systemctl enable --now nomad-client.service
EOF
done
</code></pre>
<p>This is a great opportunity to check the Nomad web UI provided by the server agent. You can find <code>unique.network.ip-address</code> in the client details for each client agent listed in the UI. The network interface used to fingerprint the network details is the interface assigned the default route on your machine. This behavior can be adjusted using the &quot;network_interface&quot; parameter in the &quot;client&quot; stanza of your configuration.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="accessing-the-client"><a class="header" href="#accessing-the-client">Accessing the client</a></h1>
<p>As a quick note before getting into the topic I want to remark that connecting to the HTTP API of the client agent using the &quot;nomad&quot; binary is not something Nomad administrators will have to do daily. Also remember that Nomad client agents are usually not installed on the same machine as Nomad server agents. This is a self-made problem.</p>
<p>Commands related to the Nomad client agent will not work as &quot;NOMAD_ADDR&quot; is set to the local servers agent IP address:</p>
<pre><code class="language-bash">root@nomad-1:~# nomad node status -self
Nomad not running in client mode
</code></pre>
<p>Setting the value of &quot;NOMAD_ADDR&quot; to a clients IP binding does not result in a successful response either:</p>
<pre><code class="language-bash">root@nomad-1:~# NOMAD_ADDR=https://10.200.200.2:4646
root@nomad-1:~# nomad node status -self
Error querying agent info: failed querying self endpoint: Get &quot;https://10.200.200.2:4646/v1/agent/self&quot;: x509: cannot validate certificate for 10.200.200.2 because it doesn't contain any IP SANs
</code></pre>
<p><strong>Use a valid hostname used in the client agents certificate.</strong></p>
<p>The &quot;nomad&quot; binary will refuse to connect to a clients IP address as we did not include any in our certificate. So let's write a tiny wrapper to access the client correctly:</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
cat &lt;&lt;'PROFILE'&gt;&gt; ~/.profile
complete -C /usr/bin/nomad nomad_client # for autocomplete
nomad_client() { local NOMAD_ADDR=https://client-1.nomad.cluster:4646; nomad &quot;${@}&quot;; }
PROFILE
EOF
done
</code></pre>
<p>Now we can query the Nomad client agent using the wrapper function:</p>
<pre><code class="language-bash">root@nomad-n:~# nomad_client node status -self -verbose
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nginx-job-file"><a class="header" href="#nginx-job-file">Nginx job file</a></h1>
<p>The Nginx job file will run as system type and spawn a worker on all Nomad hosts in the defined datacenter.</p>
<p>The provided template will create a Nginx upstream and location block for each Nomad service with the following tags:</p>
<ul>
<li>&quot;public-html&quot;</li>
<li>A key/value map converted to JSON containing &quot;path&quot; for the location block</li>
</ul>
<p>In example:</p>
<pre><code>tags = [&quot;public-http&quot;, jsonencode({
  path = &quot;/mutagen/status.json&quot;
})]
</code></pre>
<p>Each machine with a healthy service instance will be added as upstream.</p>
<hr />
<p>This job file must be passed a &quot;server_name&quot; variable value:</p>
<pre><code class="language-bash">nomad run -var server_name=nomad.debinux.de nginx.nomad
</code></pre>
<p>Alternatively a default value can be assigned on top of the job file.</p>
<pre><code class="language-bash">variable &quot;server_name&quot; {
  type = string
  #default = &quot;nomad.debinux.de&quot;
}

job &quot;lb&quot; {

  datacenters = [&quot;falkenstein&quot;]
  type = &quot;system&quot;

  # Force a refresh
  meta {
    run_uuid = &quot;${uuidv4()}&quot;
  }

  group &quot;web&quot; {

    network {
      # Using the same port for &quot;to&quot; and &quot;static&quot; is important:
      # Some web apps (like mdBook) will read the port (i.e. 8443) from the Host header and re-use it in redirects.
      # Applications may then redirect the user to the port used internally (&quot;to&quot;).
      # Redirects by DNAT are not transparent to applications like Nginx and would require some workarounds.
      # The given ports should match. Using 8443 for to _and_ static is fine.
      port &quot;http&quot; {
        to = 80
        static = 80
      }
      port &quot;https&quot; {
        to = 443
        static = 443
      }
    }

    service {
      name     = &quot;lb-http&quot;
      port     = &quot;http&quot;
      provider = &quot;nomad&quot;
    }

    service {
      name     = &quot;lb-https&quot;
      port     = &quot;https&quot;
      provider = &quot;nomad&quot;
    }

    restart {
      attempts = 2
      interval = &quot;30m&quot;
      delay = &quot;15s&quot;
      mode = &quot;fail&quot;
    }

    task &quot;nginx&quot; {

      driver = &quot;docker&quot;

      config {
        image = &quot;nginx&quot;
        ports = [&quot;http&quot;, &quot;https&quot;]
        auth_soft_fail = true
        volumes = [
          &quot;local:/etc/nginx/conf.d&quot;,
          &quot;www:/var/www&quot;,
          &quot;tls:/etc/nginx/tls.d&quot;,
        ]
      }

      logs {
        max_files     = 10
        max_file_size = 15
      }

      artifact {
        source      = &quot;https://github.com/andryyy/nomad-docs/archive/refs/heads/main.zip&quot;
        destination = &quot;www/nomad-docs&quot;
      }

      template {
        data = &lt;&lt;EOH
{{ with nomadVar &quot;nomad/jobs/lb/web/nginx&quot; }}{{ .fullchain_pem }}{{ end }}
EOH
        destination = &quot;tls/fullchain.pem&quot;
        change_mode   = &quot;signal&quot;
        change_signal = &quot;SIGHUP&quot;
      }

      template {
        data = &lt;&lt;EOH
{{ with nomadVar &quot;nomad/jobs/lb/web/nginx&quot; }}{{ .privkey_pem }}{{ end }}
EOH
        destination = &quot;tls/privkey.pem&quot;
        perms = &quot;600&quot;
        change_mode   = &quot;signal&quot;
        change_signal = &quot;SIGHUP&quot;
      }

      template {
        data = &lt;&lt;EOF
map $http_x_forwarded_proto $host_port {
  default 80;
  https 443;
}

{{ range nomadServices }}
  {{ if contains &quot;public-http&quot; .Tags }}
upstream {{ .Name }} {
    {{ range nomadService .Name }}
  server {{ .Address }}:{{ .Port }};
    {{ else }}
  server 127.0.0.1:65535;
    {{ end }}
}
  {{ end }}
{{ end }}

server {
   listen 80;
   server_name ${var.server_name};
   server_tokens off;
   return 301 https://${var.server_name}$request_uri;
}

server {
  listen 443 ssl;
  ssl_certificate /etc/nginx/tls.d/fullchain.pem;
  ssl_certificate_key /etc/nginx/tls.d/privkey.pem;
  server_tokens off;
  root /var/www;
  server_name ${var.server_name};

{{ range nomadServices }}
  {{ if contains &quot;public-http&quot; .Tags }}
    {{ $service := .Name }}
    {{ range .Tags }}
      {{ if . | regexMatch &quot;^{\&quot;.+\&quot;}$&quot; }}
        {{ with $d := . | parseJSON }}
          {{ if $d.path }}
  location {{ $d.path }} {
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP  $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    client_max_body_size 0;
    proxy_redirect off;
    proxy_pass http://{{ $service }};
            {{ if $d.limit_except }}
    limit_except {{ $d.limit_except }} {
      deny all;
    }
            {{ end }}
  }
          {{ end }}
        {{ end }}
      {{ end }}
    {{ end }}
  {{ end }}
{{ end }}

  location /docs {
    limit_except GET {
      deny all;
    }
    alias /var/www/nomad-docs/nomad-docs-main/book/book;
  }

}
EOF
        destination   = &quot;local/balancer.conf&quot;
        change_mode   = &quot;signal&quot;
        change_signal = &quot;SIGHUP&quot;
      }

      template {
        data = &lt;&lt;EOF
&lt;!DOCTYPE HTML&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&quot;utf-8&quot; /&gt;
  &lt;title&gt;LB&lt;/title&gt;
  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div id=&quot;container&quot;&gt;
&lt;pre&gt;
# https://pkg.go.dev/github.com/hashicorp/consul-template/dependency#NomadService
{{ range nomadServices }}
  Name: {{ .Name }}
  {{ range nomadService .Name }}
    Node: {{ .Node }}
    Address: {{ .Address }}
    AllocID: {{ .AllocID }}
    Datacenter: {{ .Datacenter }}
    ID: {{ .ID }}
    JobID: {{ .JobID }}
    Port: {{ .Port }}
    Tags: {{ .Tags }}
  {{ end }}
{{ end }}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
EOF
        destination   = &quot;www/services.html&quot;
        change_mode   = &quot;signal&quot;
        change_signal = &quot;SIGHUP&quot;
      }
    }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sshd-job-file"><a class="header" href="#sshd-job-file">SSHd job file</a></h1>
<p><strong>This job does require levant.</strong></p>
<p>Levant was used to create a dynamic amount of tasks depending on the configuration found in <code>variables.json</code>.</p>
<p>Moreover Levant offers the ability to outsource files to be included when rendering the job. This greatly improves the readability of the job file.</p>
<p>Structure:</p>
<pre><code class="language-bash">sshd/
 includes
    permfix
    prepare.sh
 run_sshd.sh
 set_var.sh
 sshd.nomad
 variables.json
</code></pre>
<h2 id="variablesjson"><a class="header" href="#variablesjson">variables.json</a></h2>
<p><strong>Filename</strong>: <code>sshd/variables.json</code></p>
<p><strong>Important</strong>: Task names must be unique.</p>
<pre><code class="language-bash">{
  &quot;tasks&quot;: [
    {
      &quot;name&quot;: &quot;sshd_1000_1000&quot;,
      &quot;uid&quot;: &quot;1000&quot;,
      &quot;gid&quot;: &quot;1000&quot;,
      &quot;umask&quot;: &quot;0077&quot;
    },
    {
      &quot;name&quot;: &quot;sshd_999_999&quot;,
      &quot;uid&quot;: &quot;999&quot;,
      &quot;gid&quot;: &quot;999&quot;,
      &quot;umask&quot;: &quot;0022&quot;
    }
  ]
}

</code></pre>
<h2 id="set_varsh"><a class="header" href="#set_varsh">set_var.sh</a></h2>
<p><strong>Filename</strong>: <code>sshd/set_var.sh</code></p>
<pre><code class="language-bash"># All tasks of group sync of job sshd have access
ssh-keygen -q -t ed25519 -f id_ed25519 -N &quot;&quot; &lt;&lt;&lt;y &gt;/dev/null 2&gt;&amp;1
for varpath in nomad/jobs/sync/mutagen/project nomad/jobs/sshd/sync; do
  MODIFY_INDEX=$(nomad var get -out=json ${varpath} 2&gt;/dev/null| jq -r .ModifyIndex)
  jq --null-input \
    --arg id_ed25519 &quot;$(cat id_ed25519)&quot; \
    --arg id_ed25519_pub &quot;$(cat id_ed25519.pub)&quot; \
    '{&quot;Items&quot;: {&quot;id_ed25519&quot;: $id_ed25519, &quot;id_ed25519_pub&quot;: $id_ed25519_pub}}' | \
      nomad var put -in=json -check-index=${MODIFY_INDEX} ${varpath} -
done
rm id_ed25519.pub id_ed25519
</code></pre>
<h2 id="includespermfix"><a class="header" href="#includespermfix">includes/permfix</a></h2>
<p>This script is called by Mutagen to fix permissions on scan problems.
Besides this script having all necessary fail-safes, Mutagen will also run some sanity checks before sending a path to this script.</p>
<p><strong>Filename</strong>: <code>sshd/includes/permfix</code></p>
<pre><code class="language-bash">#!/bin/bash

# Tests

## permfix one two ; echo $?
## assert 1

## permfix nonsense123 ; echo $?
## assert 2

## permfix $(echo -n '/shared-data/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/i do not exist' | base64) ; echo $?
## permfix $(echo -n '/shared-data/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/\ &quot;\&quot;  &quot; &quot;&quot;  $$!echo' | base64)
## assert 3

## permfix $(echo -n '/shared-data/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/../../etc/passwd' | base64) ; echo $?
## assert 4

## permfix &quot;&quot;
## assert 5

# We allow a single argument
if [ $# -ne 1 ]; then
  exit 1
fi

# Argument must not be empty
if [[ ! -z &quot;$1&quot; ]]; then
  # Argument must be base64 encoded
  if ! echo &quot;$1&quot; | base64 -d &gt; /dev/null; then
    exit 2
  fi
  # Wrap decoded argument in quotes
  OBJ=&quot;$(echo &quot;$1&quot; | base64 -d)&quot;
  # Path must start with /shared-data/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}
  if [[ $(readlink  -f &quot;$OBJ&quot;) =~ ^/shared-data/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }} ]]; then
    # Must be directory or file
    if [ -d &quot;$OBJ&quot; ]; then
      /bin/chmod --reference=/references/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/ref.dir &quot;$OBJ&quot;
      /bin/chown --reference=/references/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/ref.dir &quot;$OBJ&quot;
    elif [ -f &quot;$OBJ&quot; ]; then
      /bin/chmod --reference=/references/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/ref.file &quot;$OBJ&quot;
      /bin/chown --reference=/references/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/ref.file &quot;$OBJ&quot;
    else
      exit 3
    fi
  else
    exit 4
  fi
else
  exit 5
fi
</code></pre>
<h2 id="includespreparesh"><a class="header" href="#includespreparesh">includes/prepare.sh</a></h2>
<p>This script will prepare the environment.</p>
<p>It does read the given umask value and create an ACL file to be applied on the desired path, that is derived by the UID and GID, i.e. <code>/shared-data/$uid_$gid</code>.</p>
<p>Moreover a reference file and directory is installed for &quot;permfix&quot; to use when fixing scan problems.</p>
<p>The user &quot;user&quot; will only be allowed to call the permfix script using sudo.</p>
<p><strong>Filename</strong>: <code>sshd/includes/prepare.sh</code></p>
<pre><code class="language-bash">#!/bin/bash

# We need -E for our trap to trigger also in functions
set -eE

# Kill self on any error
trap 'kill 1' ERR

# This can be run everytime as it is simply too fast to be outsourced
echo &quot;Installing packages&quot;
apk add acl findutils

umask {{ env &quot;UMASK&quot; }}
echo &quot;Current umask: $(umask)&quot;
echo &quot;Current umask, sybolic: $(umask -S)&quot;

#  Convert umask to acl syntax
u=$(umask -S | cut -d',' -f1 | cut -d'=' -f2)
g=$(umask -S | cut -d',' -f2 | cut -d'=' -f2)
o=$(umask -S | cut -d',' -f3 | cut -d'=' -f2)
acl=

for x in &quot;$u&quot; &quot;$g&quot; &quot;$o&quot;; do
acl=$acl$(printf '%s%s%s' \
  $([[ $x == *&quot;r&quot;* ]] &amp;&amp; echo r || echo -) \
  $([[ $x == *&quot;w&quot;* ]] &amp;&amp; echo w || echo -) \
  $([[ $x == *&quot;x&quot;* ]] &amp;&amp; echo x || echo -)
)&quot; &quot;
done

create_acl_file() {
  [ $# -ne 3 ] &amp;&amp; return 1
  echo &quot;Creating ACL file:&quot;
  cat &lt;&lt;EOF | tee /acl_{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}
# file: /shared-data/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}
# owner: {{ env &quot;PUID&quot; }}
# group: {{ env &quot;PGID&quot; }}
user::$1
group::$2
other::$3
default:user::$1
default:group::$2
default:other::$3
EOF
}

create_acl_file $acl

echo &quot;Creating reference directory and file&quot;
install -m 755 --owner=user --group=user \
  -d /references/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}
sudo -u user -i &lt;&lt;EOF
umask {{ env &quot;UMASK&quot; }}
touch /references/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/ref.file
mkdir /references/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}/ref.dir
EOF

mkdir -p &quot;/shared-data/{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}&quot;
cd /
setfacl --restore=/acl_{{ env &quot;PUID&quot; }}_{{ env &quot;PGID&quot; }}

echo &quot;Allowing user to run permfix script as root&quot;
echo 'user ALL=(ALL) NOPASSWD: /usr/local/sbin/permfix *' &gt;&gt; /etc/sudoers.d/user

{{ $task_name_var := (print &quot;NOMAD_PORT_&quot; (env &quot;NOMAD_TASK_NAME&quot;)) }}
echo &quot;Set SSHd listen address to dynamic port {{ env $task_name_var }}&quot;
sed -i 's/2222/{{ env $task_name_var }}/' /etc/s6-overlay/s6-rc.d/svc-openssh-server/run
sed -i 's/2222/{{ env $task_name_var }}/' /var/run/service/svc-openssh-server/run.user
</code></pre>
<h2 id="sshdnomad"><a class="header" href="#sshdnomad">sshd.nomad</a></h2>
<p><strong>Filename</strong>: <code>sshd/sshd.nomad</code></p>
<pre><code class="language-bash">

job &quot;sshd&quot; {

  datacenters = [&quot;falkenstein&quot;]
  type = &quot;system&quot;

  group &quot;sync&quot; {

    volume &quot;shared-data&quot; {
      type            = &quot;host&quot;
      source          = &quot;shared-data&quot;
      read_only       = false
    }


    network {
      mode = &quot;bridge&quot;
[[- $variables := (fileContents &quot;variables.json&quot; | parseJSON ) -]]
[[- range $i, $task := $variables.tasks -]]
[[- if ne $i 0 -]][[- println &quot;&quot; -]][[- end ]]
      port &quot;[[ $task.name ]]&quot; {
        host_network = &quot;nomad-clients&quot;
        to = -1
      }
[[- end ]]
    }

[[- range $i, $task := $variables.tasks -]]
[[- if ne $i 0 -]][[- println &quot;&quot; -]][[- end ]]
    task &quot;[[ $task.name ]]&quot; {

      logs {
        max_files     = 10
        max_file_size = 15
      }

      service {
        name     = &quot;sshd&quot;
        port     = &quot;[[ $task.name ]]&quot;
        provider = &quot;nomad&quot;
        tags = [&quot;[[ $task.uid ]]:[[ $task.gid ]]&quot;]
      }

      driver = &quot;docker&quot;

      env {
        PUID = &quot;[[ $task.uid ]]&quot;
        PGID = &quot;[[ $task.gid ]]&quot;
        UMASK = &quot;[[ $task.umask ]]&quot;
        # Overwritten in prepare script
        SUDO_ACCESS = &quot;false&quot;
        TZ = &quot;Europe/Berlin&quot;
        USER_NAME = &quot;user&quot;
        PUBLIC_KEY_FILE = &quot;/secrets/authorized_keys&quot;
      }

      config {
        image = &quot;linuxserver/openssh-server&quot;
        ports = [&quot;[[ $task.name ]]&quot;]
        volumes = [
          &quot;ssh:/config/.ssh&quot;,
          &quot;custom-init-d:/custom-cont-init.d&quot;,
          &quot;usr-local-sbin:/usr/local/sbin&quot;,
        ]
      }

      template {
        data = &lt;&lt;EOH
[[ fileContents &quot;includes/prepare.sh&quot; ]]
EOH
        destination = &quot;custom-init-d/prepare.sh&quot;
        perms = &quot;755&quot;
      }

      template {
        data = &lt;&lt;EOH
[[ fileContents &quot;includes/permfix&quot; ]]
EOH
        destination = &quot;usr-local-sbin/permfix&quot;
        perms = &quot;755&quot;
      }

      template {
        data = &lt;&lt;EOH
{{ with nomadVar &quot;nomad/jobs/sshd/sync&quot; }}{{ .id_ed25519_pub }}{{ end }}
EOH
        destination = &quot;secrets/authorized_keys&quot;
        change_mode   = &quot;restart&quot;
      }

      volume_mount {
        volume      = &quot;shared-data&quot;
        destination = &quot;/shared-data&quot;
      }
    }
[[- end ]]
  }
}
</code></pre>
<h2 id="run_sshdsh"><a class="header" href="#run_sshdsh">run_sshd.sh</a></h2>
<p>The deploy script is calling <code>levant deploy</code>, which is pretty close to <code>nomad run</code>.</p>
<p>The script reads the required variables using the <code>-var-file</code> parameter and also adds the ability to call the script with extra parameters:</p>
<pre><code class="language-bash"># Deploy
./run_sshd.sh
# Force-deploy
./run_sshd.sh -force
</code></pre>
<p><strong>Filename</strong>: <code>sshd/run_sshd.sh</code></p>
<pre><code class="language-bash">levant deploy ${@} -var-file=variables.json
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mutagen-job-file"><a class="header" href="#mutagen-job-file">Mutagen job file</a></h1>
<p>This job file must be passed &quot;telegram_bot_token&quot; and &quot;telegram_chat_id&quot; variables <strong>if Telegram notifications should be enabled</strong>:</p>
<pre><code class="language-bash">nomad run -var telegram_bot_token=&quot;123:xyz-xyz&quot; -var telegram_chat_id=&quot;-123&quot; sync.nomad
</code></pre>
<p>Alternatively default values can be assigned on top of the job file.</p>
<p><strong>Important</strong>: These variables will not be stored encrypted.</p>
<pre><code class="language-bash">job &quot;sync&quot; {

  update {
    max_parallel  = 1
    health_check  = &quot;checks&quot;
    auto_revert = true
  }

  datacenters = [&quot;falkenstein&quot;]
  type = &quot;service&quot;

  group &quot;mutagen&quot; {

    count = 1

    volume &quot;shared-data&quot; {
      type            = &quot;host&quot;
      source          = &quot;shared-data&quot;
      read_only       = false
    }

    network {
      mode = &quot;bridge&quot;
      port &quot;mutagen_flask&quot; {
        host_network = &quot;nomad-clients&quot;
        to = 8082
      }
    }

    service {
      name     = &quot;mutagen&quot;
      port     = &quot;mutagen_flask&quot;
      provider = &quot;nomad&quot;
      tags = [&quot;public-http&quot;, &quot;{\&quot;path\&quot;: \&quot;/mutagen/\&quot;, \&quot;limit_except\&quot;: \&quot;GET\&quot;}&quot;]
      check {
        name     = &quot;mutagen_probe&quot;
        type     = &quot;http&quot;
        interval = &quot;10s&quot;
        timeout  = &quot;2s&quot;
        path  = &quot;/mutagen/health.json&quot;
        port  = &quot;mutagen_flask&quot;
      }
    }

    task &quot;project&quot; {

      env {
        TELEGRAM_BOT_TOKEN = &quot;[[ .telegram_bot_token ]]&quot;
        CHAT_ID = &quot;[[ .telegram_chat_id ]]&quot;
        TZ = &quot;Europe/Berlin&quot;
      }

      volume_mount {
        volume      = &quot;shared-data&quot;
        destination = &quot;/shared-data&quot;
      }

      driver = &quot;docker&quot;
      config {
        image = &quot;python:3&quot;
        ports = [&quot;mutagen_flask&quot;]
        command = &quot;/bin/bash&quot;
        args = [&quot;-c&quot;, &lt;&lt;EOF
[[ fileContents &quot;init.sh&quot; ]]
EOF
]
        volumes = [
          &quot;ssh:/root/.ssh&quot;,
        ]
      }

      template {
        data = &lt;&lt;EOF
CHAT_ID={{ env &quot;CHAT_ID&quot; }}
TELEGRAM_BOT_TOKEN={{ env &quot;TELEGRAM_BOT_TOKEN&quot; }}
EOF
        destination = &quot;secrets/file.env&quot;
        change_mode = &quot;restart&quot;
        env = true
      }

      template {
        data = &lt;&lt;EOF
[[ fileContents &quot;mutagen.yml&quot; ]]
EOF
        destination = &quot;local/mutagen.yml&quot;
        change_mode = &quot;signal&quot;
        change_signal = &quot;SIGUSR1&quot;
      }

      template {
        data = &lt;&lt;EOF
[[ fileContents &quot;flask-app.py&quot; ]]
EOF
        destination   = &quot;local/flask-app.py&quot;
        change_mode   = &quot;restart&quot;
      }

      template {
        data = &lt;&lt;EOH
{{ with nomadVar &quot;nomad/jobs/sync/mutagen/project&quot; }}{{ .id_ed25519 }}{{ end }}
EOH
        destination = &quot;secrets/id_ed25519&quot;
        change_mode = &quot;noop&quot;
        perms = &quot;600&quot;
      }

      template {
        data = &lt;&lt;EOH
Host *
  IdentityFile /secrets/id_ed25519
  StrictHostKeyChecking no
EOH
        destination = &quot;ssh/config&quot;
        change_mode   = &quot;noop&quot;
        perms = &quot;600&quot;
      }

      artifact {
        source      = &quot;https://github.com/mutagen-io/mutagen/releases/download/v0.16.3/mutagen_linux_amd64_v0.16.3.tar.gz&quot;
        destination = &quot;local&quot;
      }
    }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="databases--caches-job-file"><a class="header" href="#databases--caches-job-file">Databases &amp; caches job file</a></h1>
<pre><code class="language-bash">job &quot;databases&quot; {
  datacenters = [&quot;falkenstein&quot;]
  type = &quot;service&quot;

  group &quot;cache&quot; {
    count = 1

    restart {
      attempts = 5
      interval = &quot;1m&quot;
      delay = &quot;10s&quot;
      mode = &quot;fail&quot;
    }

    network {
      mode = &quot;bridge&quot;
      port &quot;redis&quot; {
        host_network = &quot;nomad-clients&quot;
        to = 6379
      }
    }

    service {
      name     = &quot;redis&quot;
      port     = &quot;redis&quot;
      provider = &quot;nomad&quot;
      check {
        name     = &quot;redis_probe&quot;
        type     = &quot;tcp&quot;
        interval = &quot;10s&quot;
        timeout  = &quot;2s&quot;
      }
    }

    volume &quot;shared-data&quot; {
      type = &quot;host&quot;
      source = &quot;shared-data&quot;
      read_only = false
    }

    task &quot;redis&quot; {
      driver = &quot;docker&quot;
      env {
        TZ = &quot;Europe/Berlin&quot;
      }
      config {
        image = &quot;redis:latest&quot;
        ports = [&quot;redis&quot;]
        args = [&quot;--dir&quot;, &quot;/shared-data/999_999&quot;]
      }
      volume_mount {
        volume      = &quot;shared-data&quot;
        destination = &quot;/shared-data&quot;
      }
    }
  }

  group &quot;index&quot; {
    count = 1

    restart {
      attempts = 5
      interval = &quot;1m&quot;
      delay = &quot;10s&quot;
      mode = &quot;fail&quot;
    }

    network {
      mode = &quot;bridge&quot;
      port &quot;sonic&quot; {
        host_network = &quot;nomad-clients&quot;
        to = 1491
      }
    }

    service {
      name     = &quot;sonic&quot;
      port     = &quot;sonic&quot;
      provider = &quot;nomad&quot;
      check {
        name     = &quot;sonic_probe&quot;
        type     = &quot;tcp&quot;
        interval = &quot;10s&quot;
        timeout  = &quot;2s&quot;
      }
    }

    volume &quot;shared-data&quot; {
      type = &quot;host&quot;
      source = &quot;shared-data&quot;
      read_only = false
    }

    task &quot;sonic&quot; {
      driver = &quot;docker&quot;
      env {
        TZ = &quot;Europe/Berlin&quot;
      }
      config {
        image = &quot;valeriansaliou/sonic:latest&quot;
        ports = [&quot;sonic&quot;]
        command = &quot;sonic&quot;
        args = [&quot;-c&quot;, &quot;/etc/sonic/sonic.cfg&quot;]
        volumes = [
          &quot;sonic:/etc/sonic&quot;,
        ]
      }
      volume_mount {
        volume      = &quot;shared-data&quot;
        destination = &quot;/shared-data&quot;
      }
      user = &quot;1000:1000&quot;
      template {
        data = &lt;&lt;EOF
{{ with nomadVar &quot;nomad/jobs/databases/index/sonic&quot; }}{{ .sonic_env }}{{ end }}
EOF
        destination = &quot;local/file.env&quot;
        perms = &quot;600&quot;
        uid = 1000
        gid = 1000
        env = true
      }
      template {
        data = &lt;&lt;EOF
[server]
log_level = &quot;info&quot;

[channel]
inet = &quot;0.0.0.0:1491&quot;
tcp_timeout = 300
auth_password = &quot;{{ `${env.SONIC_SECRET}` }}&quot;

[channel.search]
query_limit_default = 50
query_limit_maximum = 200
query_alternates_try = 4

suggest_limit_default = 5
suggest_limit_maximum = 20

list_limit_default = 100
list_limit_maximum = 500

[store]
[store.kv]
path = &quot;/shared-data/1000_1000/sonic/store/kv/&quot;
retain_word_objects = 1000

[store.kv.pool]
inactive_after = 1800

[store.kv.database]
flush_after = 500
compress = true
parallelism = 4
max_files = 100
max_compactions = 1
max_flushes = 1
write_buffer = 32768
write_ahead_log = true

[store.fst]
path = &quot;/shared-data/1000_1000/sonic/store/fst/&quot;

[store.fst.pool]
inactive_after = 300

[store.fst.graph]
consolidate_after = 180
max_size = 2048
max_words = 250000
EOF
        destination = &quot;sonic/sonic.cfg&quot;
      }
    }
  }

  group &quot;mariadb&quot; {
    count = 1

    volume &quot;shared-data&quot; {
      type = &quot;host&quot;
      source = &quot;shared-data&quot;
      read_only = false
    }

    network {
      mode = &quot;bridge&quot;
      port &quot;mariadb&quot; {
        host_network = &quot;nomad-clients&quot;
        to = 3306
      }
    }

    service {
      name     = &quot;mariadb&quot;
      port     = &quot;mariadb&quot;
      provider = &quot;nomad&quot;
      check {
        name     = &quot;mariadb_probe&quot;
        type     = &quot;tcp&quot;
        interval = &quot;10s&quot;
        timeout  = &quot;2s&quot;
      }
    }

    task &quot;sync-check-prestart&quot; {
      driver = &quot;exec&quot;
      config {
        command = &quot;/bin/sh&quot;
        args    = [&quot;-c&quot;, &lt;&lt;EOF
i=0
while true; do
  i=$((i+1))
  curl --output /dev/null --silent --head -m 3 --fail $MUTAGEN_HTTP &amp;&amp; { echo &quot;Loop $i: Mutagen is healthy.&quot;; exit 0; }
  [ $i -gt 10 ] &amp;&amp; { echo &quot;Loop $i: Mutagen is unhealthy, giving up.&quot;; exit 1; }
  echo &quot;Loop $i: Mutagen is unhealthy, trying again...&quot; ; sleep 3 ;
done
EOF
]
      }
      template {
        data = &lt;&lt;EOF
MUTAGEN_HTTP=http://{{ with nomadService &quot;mutagen&quot; }}{{ with index . 0 }}{{ .Address }}:{{ .Port }}{{ end }}{{ end }}/mutagen/health.json
EOF
        destination = &quot;local/file.env&quot;
        env = true
      }

      lifecycle {
        hook    = &quot;prestart&quot;
        sidecar = false
      }
    }

    task &quot;sync-check-poststop&quot; {
      driver = &quot;exec&quot;
      config {
        command = &quot;/bin/sh&quot;
        args    = [&quot;-c&quot;, &lt;&lt;EOF
echo &quot;Flushing Mutagen cache...&quot;
echo -n &quot;Return code: &quot;
curl $MUTAGEN_HTTP
EOF
]
      }
      template {
        data = &lt;&lt;EOF
MUTAGEN_HTTP=http://{{ with nomadService &quot;mutagen&quot; }}{{ with index . 0 }}{{ .Address }}:{{ .Port }}{{ end }}{{ end }}/mutagen/flush
EOF
        destination = &quot;local/file.env&quot;
        env = true
      }

      lifecycle {
        hook    = &quot;poststop&quot;
        sidecar = false
      }
    }

    task &quot;mariadb-db1&quot; {
      driver = &quot;docker&quot;
      env {
        TZ = &quot;Europe/Berlin&quot;
        MARIADB_AUTO_UPGRADE = &quot;true&quot;
      }
      config {
        image = &quot;mariadb:10&quot;
        ports = [&quot;mariadb&quot;]
        volumes = [
          &quot;config:/config&quot;,
          &quot;mysql-conf.d:/etc/mysql/conf.d&quot;
        ]
      }
      logs {
        max_files     = 10
        max_file_size = 15
      }
      template {
        data = &lt;&lt;EOH
# todo: use task name
{{ with nomadVar &quot;nomad/jobs/databases/mariadb/mariadb&quot; }}{{ .mariadb_env }}{{ end }}
EOH
        destination = &quot;secrets/env&quot;
        change_mode   = &quot;restart&quot;
        env = true
      }
      volume_mount {
        volume      = &quot;shared-data&quot;
        destination = &quot;/shared-data&quot;
      }
#       template {
#         data = &lt;&lt;EOH
# # Here if you need me
# #/bin/sh
# echo &quot;Starting backup...&quot;
# if [ -d &quot;/shared-data/mariadb&quot; ]; then
#   echo &quot;Found directory&quot;
#   source /config/env
#   mariabackup -uroot -p$MYSQL_ROOT_PASSWORD --backup--target-dir=/shared-data/mariadb-backup
#   mariabackup -uroot -p$MYSQL_ROOT_PASSWORD --prepare --target-dir=/shared-data/mariadb-backup
# fi
# EOH
#         destination = &quot;custom-init-d/prepare&quot;
#         perms = &quot;755&quot;
#       }
      template {
        data = &lt;&lt;EOH
[mysqld]
datadir=/shared-data/999_999/db1
EOH
        destination = &quot;mysql-conf.d/custom.cnf&quot;
        perms = &quot;755&quot;
      }
    }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="damon"><a class="header" href="#damon">Damon</a></h1>
<p>There currently isn't an official release nor a pre-built binary to download, so I compiled it myself:</p>
<ul>
<li>linux/amd64, <a href="tools/damon-v0.1.0-dev">damon-v0.1.0-dev</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="wander"><a class="header" href="#wander">Wander</a></h1>
<p><a href="https://github.com/robinovitch61/wander">Wander</a> is a great alternative to &quot;damon&quot; by Hashicorp.</p>
<p>I found it to be more convenient compared to &quot;damon&quot;.</p>
<p>The exec functionality does, as of writing this article in Jan 2023, not work with TLS-enabled Nomad stacks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lazydocker"><a class="header" href="#lazydocker">Lazydocker</a></h1>
<p><a href="https://github.com/jesseduffield/lazydocker">Lazydocker</a> is a TUI for Docker to quickly access and manage container logs, stats, volumes, and even allows to quickly enter a running container with a shell.</p>
<p>The latest version can be found <a href="https://github.com/jesseduffield/lazydocker/releases/latest">here</a>.</p>
<pre><code class="language-bash">wget -O lazydocker.tar.gz https://github.com/jesseduffield/lazydocker/releases/download/v0.20.0/lazydocker_0.20.0_Linux_x86_64.tar.gz
tar xfz lazydocker.tar.gz --directory=.
for nomad in $(jq -r keys[] ~/nomad-env.json); do
  scp lazydocker $nomad:/usr/local/sbin/lazydocker
done
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-enter-containers"><a class="header" href="#quick-enter-containers">Quick enter containers</a></h1>
<p>This script refers to <code>nomad-env.json</code>.</p>
<pre><code class="language-bash">for nomad in $(jq -r keys[] ~/nomad-env.json); do
  ssh $nomad bash &lt;&lt;'EOF'
cat &lt;&lt;'ALIASES'&gt;&gt; ~/.bashrc
alias nomad_sshd='docker exec -it $(docker ps -qf name=sshd) bash'
alias nomad_sync='docker exec -it $(docker ps -qf name=sync) bash'
alias nomad_nginx='docker exec -it $(docker ps -qf name=nginx) bash'
alias nomad_mariadb='docker exec -it $(docker ps -qf name=mariadb) bash'
ALIASES
EOF
done
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nomad-services"><a class="header" href="#nomad-services">Nomad services</a></h1>
<p>See <a href="https://pkg.go.dev/github.com/hashicorp/consul-template/dependency#NomadService">https://pkg.go.dev/github.com/hashicorp/consul-template/dependency#NomadService</a> for a reference of available keys.</p>
<p><a href="server_debug//services.html">Nomad service overview</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="sidebar.js"></script>

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
